# -*- coding: utf-8 -*-
"""Pyspark_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lK6PD-e3xcAP_X0rc1WWYBCnDjE_sh90

#instalacion
"""

!pip install pyspark
!pip install pyspark_dist_explore
import pyspark
from pyspark import SparkConf,SparkContext
from pyspark.sql import SparkSession
import matplotlib.pyplot as plt
import pandas as pd
from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
from pyspark.sql.types import FloatType
from pyspark_dist_explore import hist
import matplotlib.pyplot as plt
from pyspark.ml.feature import VectorAssembler, StandardScaler

from pyspark.sql import functions as F
from pyspark.sql.functions import lit
from pyspark.sql.functions import concat_ws
from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id, row_number
from pyspark.sql import Window

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from pyspark import SparkConf, SparkContext
from pyspark.sql import  SQLContext

from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import udf, col

from pyspark.ml.regression import LinearRegression
from pyspark.mllib.evaluation import RegressionMetrics

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql import SparkSession,SQLContext

spark = SparkSession.builder.appName("Basics").getOrCreate()
sc=spark.sparkContext
SQLContext = SQLContext(sc)

# define the schema, corresponding to a line in the csv data file.
schema = StructType([
    StructField("long", FloatType(), nullable=True),
    StructField("lat", FloatType(), nullable=True),
    StructField("medage", FloatType(), nullable=True),
    StructField("totrooms", FloatType(), nullable=True),
    StructField("totbdrms", FloatType(), nullable=True),
    StructField("pop", FloatType(), nullable=True),
    StructField("houshlds", FloatType(), nullable=True),
    StructField("medinc", FloatType(), nullable=True),
    StructField("medhv", FloatType(), nullable=True)]
)

housing_df = spark.read.csv("/content/cal_housing.data", schema=schema)
print("numero filas: ",housing_df.count())
print(housing_df.describe().show())
housing_df.show()

#unir todo el data en una columna 
featureCols = ["long","lat","totbdrms","pop","houshlds","medinc","medhv","medage","totrooms"]
assembler = VectorAssembler(inputCols=featureCols, outputCol="features") 
assembled_df = assembler.transform(housing_df)
assembled_df.show(2, truncate=False)

# Drop one column
housing_df=housing_df.drop("lat")
housing_df.show()

# Filtering by City Category "A"
housing_df.filter(housing_df.medage=="42.0").show(5)

fig, ax = plt.subplots()
hist(ax, housing_df.select("long"))

# Initialize the `standardScaler`
standardScaler = StandardScaler(inputCol="features", outputCol="features_scaled")
# Fit the DataFrame to the scaler
scaled_df = standardScaler.fit(assembled_df).transform(assembled_df)
# Inspect the result
scaled_df.select("features", "features_scaled").show(10, truncate=False)

# Split the data into train and test sets
train_data, test_data = scaled_df.randomSplit([.8,.2], seed=264)

# Initialize `lr`
lr = (LinearRegression(featuresCol='features_scaled',
                       labelCol="medhv",
                       predictionCol='predmedhv', 
                       maxIter=10,
                       regParam=0.3,
                       elasticNetParam=0.8,
                       standardization=False))

# Fit the data to the model
linearModel = lr.fit(train_data)

# Coefficients for the model
linearModel.coefficients

coeff_df = pd.DataFrame({"Feature": ["Intercept"] + featureCols, "Co-efficients": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})
coeff_df = coeff_df[["Feature", "Co-efficients"]]
coeff_df

# Generate predictions
predictions = linearModel.transform(test_data)
# Extract the predictions and the "known" correct labels
predandlabels = predictions.select("predmedhv", "medhv")
predandlabels.show()

print("RMSE: {0}".format(linearModel.summary.rootMeanSquaredError))
print("MAE: {0}".format(linearModel.summary.meanAbsoluteError))
print("R2: {0}".format(linearModel.summary.r2))

"""#Otras funciones de pyspark """

address = [
    (1,"14851 Jeffrey Rd","DE","10KG"),
    (2,"43421 Margarita St","NY","15KG"),
    (3,"13111 Siemon Ave","CA","20KG"),
    (4,"13111 Siemon Ave","CA","15KG"),
    (5,"13111 Siemon Ave","CA","15KG"),
    (6,"13111 Siemon Ave","CA","10KG"),
    (7,"13111 Siemon Ave","CA","15KG"),
    (8,"13111 Siemon Ave","CA","10KG")]
df =spark.createDataFrame(address,["id","address","state","kilos"])
df.show()

df.withColumn("bonus_percent", lit(0.3)) \
  .show()

#Add column from existing column
df.withColumn("bonus_amount_2", df.id*0.3) \
  .show()

#Add column by concatinating existing columns
df.withColumn("name", concat_ws("_","address",'state')) \
  .show()

#sample data

a= SQLContext.createDataFrame([("Dog", "Cat"), 
                               ("Cat", "Dog"),
                               ("Mouse", "Cat")],       ["Animal", "Enemy"])
a.show()

#convert list to a dataframe
rating = [5,4,1]
b = SQLContext.createDataFrame([(l,) for l in rating], ['Rating'])
b.show()

#add 'sequential' index and join both dataframe to get the final result
a = a.withColumn("row_idx", row_number().over(Window.orderBy(monotonically_increasing_id())))
b = b.withColumn("row_idx", row_number().over(Window.orderBy(monotonically_increasing_id())))

a.show()

final_df = a.join(b, a.row_idx == b.row_idx).\
             drop("row_idx")
final_df.show()

#Replace values from Dictionary
stateDic={'CA':'California','NY':'New York','DE':'Delaware'}
kilosDic={'15KG':'APTO','10KG':'APTO','20KG':'NO_APTO'}

df2=df.rdd.map(lambda x: 
    (x.id,stateDic[x.state],kilosDic[x.kilos])
).toDF(["id","state","kilos"])
df2.show()

spark.stop()